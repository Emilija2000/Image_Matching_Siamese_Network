{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image matching with Siamese networks\n",
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import utilities as utils \n",
    "from siameseDataloader import readDataFolder,dataSplits,SiameseDataset\n",
    "from siameseModel import SimpleSiameseNetwork\n",
    "from siameseLosses import ContrastiveLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.load_config()\n",
    "datadir = config['DATASET']['root']\n",
    "numclasses = config['DATASET']['numclasses']\n",
    "sameprob = config['DATASET']['sameprob']\n",
    "\n",
    "seed = config['seed']\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = readDataFolder(datadir,numclasses)\n",
    "train_split,val_split,test_split = dataSplits(dataset,0.7,0.2,0.1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "test_transforms = train_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "train_dataset = SiameseDataset(train_split,train_transforms,sameprob)\n",
    "val_dataset = SiameseDataset(val_split,test_transforms,0.5)\n",
    "test_dataset = SiameseDataset(test_split,test_transforms,0.5)\n",
    "\n",
    "datasets = {'train':train_dataset,'val':val_dataset,'test':test_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloaders\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_dataset,shuffle=True,\n",
    "                        num_workers=config['TRAIN']['numworkers'],\n",
    "                        batch_size=config['TRAIN']['batchsize']),\n",
    "    'val': DataLoader(val_dataset,shuffle=False,\n",
    "                        num_workers=config['TRAIN']['numworkers'],\n",
    "                        batch_size=config['TRAIN']['batchsize']),\n",
    "    'test': DataLoader(test_dataset,shuffle=False,\n",
    "                        num_workers=config['TRAIN']['numworkers'],\n",
    "                        batch_size=config['TRAIN']['batchsize'])\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['MODEL']['model'] == 'simple':\n",
    "    model = SimpleSiameseNetwork(distance_type=config['MODEL']['distance'],pretrained=config['MODEL']['pretrained'])\n",
    "else:\n",
    "    model = SimpleSiameseNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000], grad_fn=<DivBackward0>)\n",
      "tensor([0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "data = train_dataset[0]\n",
    "out = model(data[0].reshape(1,1,64,64),data[1].reshape(1,1,64,64))\n",
    "print(out)\n",
    "print(data[2])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ContrastiveLoss(margin=config['TRAIN']['lossmargin'])\n",
    "optimizer = optim.Adam(model.parameters(),lr = config['TRAIN']['lr'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = config['TRAIN']['device']\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history ={'train':[],'val':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0,\t train loss:2.8219,\t val loss:2.8589\n",
      "Epoch: 1,\t train loss:2.8826,\t val loss:2.9691\n",
      "Epoch: 2,\t train loss:2.8348,\t val loss:2.9444\n",
      "Epoch: 3,\t train loss:2.9302,\t val loss:3.0141\n",
      "Epoch: 4,\t train loss:2.7921,\t val loss:2.8622\n",
      "Epoch: 5,\t train loss:2.7991,\t val loss:2.9275\n",
      "Epoch: 6,\t train loss:2.8441,\t val loss:2.8127\n",
      "Epoch: 7,\t train loss:2.767,\t val loss:2.8791\n",
      "Epoch: 8,\t train loss:2.8818,\t val loss:2.968\n",
      "Epoch: 9,\t train loss:2.8238,\t val loss:3.004\n",
      "Epoch: 10,\t train loss:2.811,\t val loss:2.7666\n",
      "Epoch: 11,\t train loss:2.8279,\t val loss:2.9714\n",
      "Epoch: 12,\t train loss:2.8642,\t val loss:2.9444\n",
      "Epoch: 13,\t train loss:2.8433,\t val loss:2.8724\n",
      "Epoch: 14,\t train loss:2.8035,\t val loss:2.9151\n",
      "Epoch: 15,\t train loss:2.8269,\t val loss:2.7599\n",
      "Epoch: 16,\t train loss:2.9116,\t val loss:2.8083\n",
      "Epoch: 17,\t train loss:2.8536,\t val loss:2.7711\n",
      "Epoch: 18,\t train loss:2.9029,\t val loss:2.7565\n",
      "Epoch: 19,\t train loss:2.7793,\t val loss:2.6429\n",
      "Epoch: 20,\t train loss:2.9587,\t val loss:3.0411\n",
      "Epoch: 21,\t train loss:2.8208,\t val loss:2.8679\n",
      "Epoch: 22,\t train loss:2.8379,\t val loss:2.878\n",
      "Epoch: 23,\t train loss:2.7674,\t val loss:2.8285\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Emilija\\Desktop\\cambridge\\lecture material\\CV\\CVproject\\siamese.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Emilija/Desktop/cambridge/lecture%20material/CV/CVproject/siamese.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m loss_epoch\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Emilija/Desktop/cambridge/lecture%20material/CV/CVproject/siamese.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m count\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Emilija/Desktop/cambridge/lecture%20material/CV/CVproject/siamese.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i,(img1,img2,label) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloaders[mode]):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Emilija/Desktop/cambridge/lecture%20material/CV/CVproject/siamese.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     img1 \u001b[39m=\u001b[39m img1\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Emilija/Desktop/cambridge/lecture%20material/CV/CVproject/siamese.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     img2 \u001b[39m=\u001b[39m img2\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\Emilija\\.conda\\envs\\MLenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Emilija\\.conda\\envs\\MLenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1207\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1206\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1207\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   1208\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1209\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1210\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Emilija\\.conda\\envs\\MLenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1173\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1169\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1171\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1172\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m-> 1173\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   1174\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1175\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Emilija\\.conda\\envs\\MLenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1011\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m    999\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1008\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1011\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1012\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[0;32m   1013\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1014\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Emilija\\.conda\\envs\\MLenv\\lib\\multiprocessing\\queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[0;32m    112\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[0;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[1;32mc:\\Users\\Emilija\\.conda\\envs\\MLenv\\lib\\multiprocessing\\connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[0;32m    261\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[1;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[1;32mc:\\Users\\Emilija\\.conda\\envs\\MLenv\\lib\\multiprocessing\\connection.py:335\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_got_empty_message \u001b[39mor\u001b[39;00m\n\u001b[0;32m    333\u001b[0m             _winapi\u001b[39m.\u001b[39mPeekNamedPipe(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle)[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m    334\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 335\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(wait([\u001b[39mself\u001b[39;49m], timeout))\n",
      "File \u001b[1;32mc:\\Users\\Emilija\\.conda\\envs\\MLenv\\lib\\multiprocessing\\connection.py:884\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    881\u001b[0m                 ready_objects\u001b[39m.\u001b[39madd(o)\n\u001b[0;32m    882\u001b[0m                 timeout \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 884\u001b[0m     ready_handles \u001b[39m=\u001b[39m _exhaustive_wait(waithandle_to_obj\u001b[39m.\u001b[39;49mkeys(), timeout)\n\u001b[0;32m    885\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m     \u001b[39m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     \u001b[39mfor\u001b[39;00m ov \u001b[39min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32mc:\\Users\\Emilija\\.conda\\envs\\MLenv\\lib\\multiprocessing\\connection.py:816\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    814\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[0;32m    815\u001b[0m \u001b[39mwhile\u001b[39;00m L:\n\u001b[1;32m--> 816\u001b[0m     res \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39;49mWaitForMultipleObjects(L, \u001b[39mFalse\u001b[39;49;00m, timeout)\n\u001b[0;32m    817\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m    818\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(config['TRAIN']['numepochs']):\n",
    "    for mode in ['train','val']:\n",
    "        loss_epoch=0\n",
    "        count=0\n",
    "        for i,(img1,img2,label) in enumerate(dataloaders[mode]):\n",
    "            img1 = img1.to(device)\n",
    "            img2 = img2.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            if mode=='train':\n",
    "                model.train()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                out = model(img1,img2)\n",
    "                loss = criterion(out,label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            else:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    out = model(img1,img2)\n",
    "                    loss = criterion(out,label)\n",
    "            \n",
    "            # track total loss\n",
    "            loss_epoch = loss_epoch+loss\n",
    "            count = count + len(label)\n",
    "        \n",
    "        loss_history[mode].append(loss_epoch.item()/count)\n",
    "    print('Epoch: {},\\t train loss:{:.5},\\t val loss:{:.5}'.format(epoch,loss_history['train'][-1],loss_history['val'][-1]))\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_history['train'],'b-')\n",
    "plt.plot(loss_history['val'],'r-')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "665aa01f3af077fd5771c858d8fc10596be116db06e0fbc8e2a6e92782d15ae1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
