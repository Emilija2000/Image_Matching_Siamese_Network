{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q93QhrZMpllR"
      },
      "source": [
        "# Image matching with Siamese networks\n",
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kfLmrjV1pllV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import utilities as utils \n",
        "from siameseDataloader import readDataFolder, dataSplits, TripletDataset\n",
        "from siameseModel import MobileNetTriplet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ioGAoVNzpllW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "config = utils.load_config()\n",
        "datadir = os.path.join('data','tiny-imagenet-200-copy','train')#config['DATASET']['root']\n",
        "numclasses = config['DATASET']['numclasses']\n",
        "sameprob = config['DATASET']['sameprob']\n",
        "im_size = config['DATASET']['im_size']\n",
        "emb_size=config['MODEL']['embsize']\n",
        "\n",
        "seed = config['seed']\n",
        "np.random.seed(seed)\n",
        "\n",
        "device = config['TRAIN']['device']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI5tIw04pllX"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pS1Wv6upllX",
        "outputId": "40ca24b7-da0e-4c02-ecff-6d01a6723e9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset ImageFolder\n",
            "    Number of datapoints: 25000\n",
            "    Root location: data\\tiny-imagenet-200-copy\\train\n"
          ]
        }
      ],
      "source": [
        "dataset = readDataFolder(datadir,numclasses)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OmlUZ4bAvSyA"
      },
      "outputs": [],
      "source": [
        "train_split,val_split,test_split = dataSplits(dataset,0.8,0.2,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Cy0bMF7rpllY"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize(im_size+4),\n",
        "    transforms.RandomCrop(im_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomAffine(0.3),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
        "    transforms.Normalize([0.507, 0.4865, 0.4409], [0.2673, 0.2564, 0.2761])\n",
        "])\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize(im_size),\n",
        "    transforms.CenterCrop(im_size),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
        "    transforms.Normalize([0.507, 0.4865, 0.4409], [0.2673, 0.2564, 0.2761])\n",
        "])\n",
        "\n",
        "train_dataset = TripletDataset(train_split,train_transforms)\n",
        "val_dataset = TripletDataset(val_split,test_transforms)\n",
        "test_dataset = TripletDataset(test_split,test_transforms)\n",
        "\n",
        "datasets = {'train':train_dataset,'val':val_dataset,'test':test_dataset}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MTWN4JlQpllY"
      },
      "outputs": [],
      "source": [
        "#dataloaders\n",
        "dataloaders = {\n",
        "    'train': DataLoader(train_dataset,shuffle=True,\n",
        "                        num_workers=config['TRAIN']['numworkers'],\n",
        "                        batch_size=config['TRAIN']['batchsize']),\n",
        "    'val': DataLoader(val_dataset,shuffle=False,\n",
        "                        num_workers=config['TRAIN']['numworkers'],\n",
        "                        batch_size=config['TRAIN']['batchsize']),\n",
        "    'test': DataLoader(test_dataset,shuffle=False,\n",
        "                        num_workers=config['TRAIN']['numworkers'],\n",
        "                        batch_size=config['TRAIN']['batchsize'])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 3, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "for i,data in enumerate(dataloaders['train']):\n",
        "    print(data[0].shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRwIBybxpllZ"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcLN38fZpllZ",
        "outputId": "8c1bb3ef-8af5-45ee-e365-86a80319bdd8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\Emilija/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n"
          ]
        }
      ],
      "source": [
        "model = MobileNetTriplet(config['MODEL']['distance'],embsize=emb_size,pretrained=config['MODEL']['pretrained'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4431828\n"
          ]
        }
      ],
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(pytorch_total_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlyIXdtJplla",
        "outputId": "3c41d846-7bda-497a-f9cf-fffaad2a5202"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([[ 0.0671, -0.1113,  0.0613, -0.0598,  0.0864,  0.2344,  0.0288,  0.0850,\n",
            "         -0.1108,  0.0588,  0.0656,  0.0956,  0.1192,  0.1203,  0.0472,  0.0315,\n",
            "          0.0266,  0.1176,  0.0608, -0.1547]], grad_fn=<AddmmBackward0>), tensor([[ 0.0648, -0.1174,  0.0603, -0.0589,  0.0839,  0.2338,  0.0263,  0.0824,\n",
            "         -0.1131,  0.0615,  0.0627,  0.0960,  0.1219,  0.1171,  0.0466,  0.0290,\n",
            "          0.0303,  0.1132,  0.0636, -0.1523]], grad_fn=<AddmmBackward0>), tensor([[ 0.0673, -0.1120,  0.0625, -0.0596,  0.0865,  0.2323,  0.0253,  0.0836,\n",
            "         -0.1071,  0.0575,  0.0658,  0.0984,  0.1180,  0.1185,  0.0506,  0.0289,\n",
            "          0.0263,  0.1131,  0.0600, -0.1522]], grad_fn=<AddmmBackward0>))\n"
          ]
        }
      ],
      "source": [
        "data = train_dataset[400]\n",
        "out = model(data[0].reshape(1,3,im_size,im_size),data[1].reshape(1,3,im_size,im_size),data[2].reshape(1,3,im_size,im_size))\n",
        "print(out)\n",
        "#print(criterion(out,data[2]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAgPodwZpllb"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "#model.to('cpu')\n",
        "#model.load_state_dict(model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = torch.nn.TripletMarginWithDistanceLoss(margin = config['TRAIN']['lossmargin'], distance_function=model.distance)\n",
        "#config['TRAIN']['lr'] =0.0005\n",
        "optimizer = optim.Adam(model.parameters(),lr = config['TRAIN']['lr'],weight_decay=0.03)\n",
        "#scheduler = StepLR(optimizer, step_size=10, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "caaBjVTzpllb"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jPpgvdHTpllc"
      },
      "outputs": [],
      "source": [
        "loss_history ={'train':[],'val':[]}\n",
        "minloss=10000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = model.state_dict().copy() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMYzWIedpllc",
        "outputId": "3b82562b-596a-4f37-f113-3df4505962e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0,\t train loss:0.038819,\t val loss:0.032922\n",
            "Epoch: 1,\t train loss:0.030971,\t val loss:0.029884\n",
            "Epoch: 2,\t train loss:0.028365,\t val loss:0.029322\n",
            "Epoch: 3,\t train loss:0.026906,\t val loss:0.02971\n",
            "Epoch: 4,\t train loss:0.026626,\t val loss:0.027626\n",
            "Epoch: 5,\t train loss:0.025487,\t val loss:0.028355\n",
            "Epoch: 6,\t train loss:0.02447,\t val loss:0.026417\n",
            "Epoch: 7,\t train loss:0.024221,\t val loss:0.026369\n",
            "Epoch: 8,\t train loss:0.023905,\t val loss:0.026249\n",
            "Epoch: 9,\t train loss:0.023694,\t val loss:0.0259\n",
            "Epoch: 10,\t train loss:0.023069,\t val loss:0.02673\n",
            "Epoch: 11,\t train loss:0.023136,\t val loss:0.025964\n",
            "Epoch: 12,\t train loss:0.022258,\t val loss:0.027267\n",
            "Epoch: 13,\t train loss:0.023169,\t val loss:0.027505\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Emilija\\Desktop\\cambridge\\lecture material\\CV\\Image_Matching_Siamese_Network\\siamese_triplet.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emilija/Desktop/cambridge/lecture%20material/CV/Image_Matching_Siamese_Network/siamese_triplet.ipynb#X32sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(out1,out2,out3)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emilija/Desktop/cambridge/lecture%20material/CV/Image_Matching_Siamese_Network/siamese_triplet.ipynb#X32sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Emilija/Desktop/cambridge/lecture%20material/CV/Image_Matching_Siamese_Network/siamese_triplet.ipynb#X32sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emilija/Desktop/cambridge/lecture%20material/CV/Image_Matching_Siamese_Network/siamese_triplet.ipynb#X32sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emilija/Desktop/cambridge/lecture%20material/CV/Image_Matching_Siamese_Network/siamese_triplet.ipynb#X32sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     model\u001b[39m.\u001b[39meval()\n",
            "File \u001b[1;32mc:\\Users\\Emilija\\.conda\\envs\\MLenv\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Emilija\\.conda\\envs\\MLenv\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Emilija\\.conda\\envs\\MLenv\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[0;32m    139\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 141\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[0;32m    142\u001b[0m            grads,\n\u001b[0;32m    143\u001b[0m            exp_avgs,\n\u001b[0;32m    144\u001b[0m            exp_avg_sqs,\n\u001b[0;32m    145\u001b[0m            max_exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m            state_steps,\n\u001b[0;32m    147\u001b[0m            amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    148\u001b[0m            beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    149\u001b[0m            beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    150\u001b[0m            lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    151\u001b[0m            weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m            eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m            maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    154\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
            "File \u001b[1;32mc:\\Users\\Emilija\\.conda\\envs\\MLenv\\lib\\site-packages\\torch\\optim\\_functional.py:110\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    105\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2))\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    109\u001b[0m step_size \u001b[39m=\u001b[39m lr \u001b[39m/\u001b[39m bias_correction1\n\u001b[1;32m--> 110\u001b[0m param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "for epoch in range(config['TRAIN']['numepochs']):\n",
        "    for mode in ['train','val']:\n",
        "        loss_epoch=0\n",
        "        count=0\n",
        "        for i,(img1,img2,img3) in enumerate(dataloaders[mode]):\n",
        "            img1 = img1.to(device)\n",
        "            img2 = img2.to(device)\n",
        "            img3 = img3.to(device)\n",
        "            \n",
        "            if mode=='train':\n",
        "                model.train()\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                out1,out2,out3 = model(img1,img2,img3)\n",
        "                loss = criterion(out1,out2,out3)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "            else:\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    out1,out2,out3 = model(img1,img2,img3)\n",
        "                loss = criterion(out1,out2,out3)\n",
        "            \n",
        "            # track total loss\n",
        "            loss_epoch = loss_epoch+loss\n",
        "            count = count + img1.shape[0]\n",
        "            \n",
        "        #scheduler.step()\n",
        "        \n",
        "        loss_history[mode].append(loss_epoch.item()/count)\n",
        "    print('Epoch: {},\\t train loss:{:.5},\\t val loss:{:.5}'.format(epoch,loss_history['train'][-1],loss_history['val'][-1]))\n",
        "\n",
        "    if (loss_history['val'][-1] < minloss):\n",
        "        minloss = loss_history['val'][-1]\n",
        "        best_model = model.state_dict().copy()\n",
        "            \n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(best_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "FSjCw2Jppllc",
        "outputId": "fb64d293-3dd8-4b38-e43a-b679a603dfc3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x26e824d9720>]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure()\n",
        "plt.plot(loss_history['train'],'b-')\n",
        "plt.plot(loss_history['val'],'r-')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "model=model.to(device)\n",
        "#threshs = np.linspace(start=0,stop=config['TRAIN']['lossmargin']*1.2,num=10)\n",
        "threshs = [config['TRAIN']['lossmargin']/2]\n",
        "#threshs=[0.5]\n",
        "accs = []\n",
        "sensitivity = []\n",
        "specificity = []\n",
        "f1=[]\n",
        "\n",
        "\n",
        "for thresh in threshs:\n",
        "    pos=0\n",
        "    neg=0\n",
        "    conf = np.zeros((2,2))\n",
        "    for i,(img1,img2,label) in enumerate(dataloaders['train']):\n",
        "        img1=img1.to(device)\n",
        "        img2=img2.to(device)\n",
        "        label=label.to(device)\n",
        "        with torch.no_grad():\n",
        "            out = model(img1,img2)\n",
        "        \n",
        "        conf[1,1] += torch.matmul(label.T*1.,(out>=thresh)*1.)\n",
        "        conf[1,0] += torch.matmul(label.T*1.,(out<thresh)*1.)\n",
        "        conf[0,1] += torch.matmul((1-label).T*1.,(out>=thresh)*1.)\n",
        "        conf[0,0] += torch.matmul((1-label).T*1.,(out<thresh)*1.)\n",
        "        pos = pos + torch.sum(label)\n",
        "        neg = neg + torch.sum(1-label)\n",
        "\n",
        "    accs.append(((conf[1,1]+conf[0,0])/(pos+neg)).item())\n",
        "    sensitivity.append((conf[0,0]/neg).item()) #changed 0 and 1 labels (0 is true in the loss fcn)\n",
        "    specificity.append((conf[1,1]/pos).item())\n",
        "    f1.append(((2*conf[0,0])/(2*conf[0,0]+conf[0,1]+conf[1,0])).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(accs)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNFBMDQSgg8T"
      },
      "source": [
        "## Val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVa241BzgfUL",
        "outputId": "5b0dae45-eb97-4fc6-9663-d2348d3f1db7"
      },
      "outputs": [],
      "source": [
        "loss_epoch = 0\n",
        "count=0\n",
        "for i,(img1,img2,label) in enumerate(dataloaders['val']):\n",
        "    img1 = img1.to(device)\n",
        "    img2=img2.to(device)\n",
        "    label = label.to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(img1,img2)\n",
        "        loss = criterion(out,label)\n",
        "\n",
        "    # track total loss\n",
        "    loss_epoch = loss_epoch+loss\n",
        "    count = count + len(label)\n",
        "print('Validation loss: {}'.format(loss_epoch.item()/count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "-W8PWPoJgsYq",
        "outputId": "a414ce06-b114-4415-f2e4-622771010ff2"
      },
      "outputs": [],
      "source": [
        "# Visualise\n",
        "numex=5 #number of examples\n",
        "fig,ax = plt.subplots(2,numex,figsize=(15,5))\n",
        "for i in range(numex):\n",
        "    example = val_dataset[i]\n",
        "    utils.imshow(example[0],ax[0][i])\n",
        "    utils.imshow(example[1],ax[1][i])\n",
        "    distance = model(example[0].reshape(1,3,im_size,im_size).to(device),example[1].reshape(1,3,im_size,im_size).to(device))\n",
        "    ax[0][i].set_title('label:{},distance:{:.2}'.format(example[2].item(),distance.item()))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4Kw9s08gvs0",
        "outputId": "a1c15d2f-74c1-455f-b5e1-adadaa5926db"
      },
      "outputs": [],
      "source": [
        "#threshs = np.linspace(start=0,stop=config['TRAIN']['lossmargin']*1.2,num=11)\n",
        "threshs = [config['TRAIN']['lossmargin']/2]\n",
        "#threshs = [0.5]\n",
        "#threshs = np.linspace(start=0,stop=1,num=11)\n",
        "accs=[]\n",
        "sensitivity = []\n",
        "specificity = []\n",
        "f1=[]\n",
        "\n",
        "num=0\n",
        "for thresh in threshs:\n",
        "    pos=0\n",
        "    neg=0\n",
        "    conf = np.zeros((2,2))\n",
        "    for i,(img1,img2,label) in enumerate(dataloaders['val']):\n",
        "        img1=img1.to(device)\n",
        "        img2=img2.to(device)\n",
        "        label=label.to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            out = model(img1,img2)\n",
        "        \n",
        "        conf[1,1] += torch.matmul(label.T*1.,(out>=thresh)*1.)\n",
        "        conf[1,0] += torch.matmul(label.T*1.,(out<thresh)*1.)\n",
        "        conf[0,1] += torch.matmul((1-label).T*1.,(out>=thresh)*1.)\n",
        "        conf[0,0] += torch.matmul((1-label).T*1.,(out<thresh)*1.)\n",
        "        pos = pos + torch.sum(label)\n",
        "        neg = neg + torch.sum(1-label)\n",
        "\n",
        "    accs.append(((conf[1,1]+conf[0,0])/(pos+neg)).item())\n",
        "    sensitivity.append((conf[0,0]/neg).item()) #changed 0 and 1 labels (0 is true in the loss fcn)\n",
        "    specificity.append((conf[1,1]/pos).item())\n",
        "    f1.append(((2*conf[0,0])/(2*conf[0,0]+conf[0,1]+conf[1,0])).item())\n",
        "    \n",
        "    print(num)\n",
        "    num += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(accs)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITge6vnLg13I"
      },
      "outputs": [],
      "source": [
        "accs = np.array(accs)\n",
        "sensitivity = np.array(sensitivity)\n",
        "specificity = np.array(specificity)\n",
        "f1 = np.array(f1)\n",
        "\n",
        "m1 = np.argmax(accs)\n",
        "m2 = np.argmax(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "RksTwARrg5lm",
        "outputId": "80a677e8-2bd2-43a2-c788-672bca68c412"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(1,3,figsize=(15,4))\n",
        "ax[0].plot(threshs,accs)\n",
        "ax[0].set_xlabel('threshold')\n",
        "ax[0].set_ylabel('accuracy')\n",
        "ax[1].plot(threshs,f1)\n",
        "ax[1].set_xlabel('threshold')\n",
        "ax[1].set_ylabel('f1-score')\n",
        "ax[2].plot(1-specificity,sensitivity,'o',label='_nolabel_')\n",
        "ax[2].set_xlabel('1-specificity')\n",
        "ax[2].set_ylabel('sensitivity')\n",
        "ax[2].plot(1-specificity[m1],sensitivity[m1],'ro',label='acc thresh:{}'.format(threshs[m1]))\n",
        "ax[2].plot(1-specificity[m2],sensitivity[m2],'go',label='f1 thresh:{}'.format(threshs[m2]))\n",
        "ax[2].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxpLSmPKplld"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIlXH8jcplld",
        "outputId": "d97c5fb8-fd62-4998-b040-2b34d54eb9ce"
      },
      "outputs": [],
      "source": [
        "loss_epoch=0\n",
        "count=0\n",
        "for i,(img1,img2,label) in enumerate(dataloaders['test']):\n",
        "    with torch.no_grad():\n",
        "        out = model(img1,img2)\n",
        "        loss = criterion(out,label)\n",
        "\n",
        "    # track total loss\n",
        "    loss_epoch = loss_epoch+loss\n",
        "    count = count + len(label)\n",
        "print('Test loss: {}'.format(loss_epoch.item()/count))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "sjh3GyZNplld",
        "outputId": "dc142981-53c3-4dfb-ca3f-5764390c7578"
      },
      "outputs": [],
      "source": [
        "# Visualise\n",
        "numex=5 #number of examples\n",
        "fig,ax = plt.subplots(2,numex,figsize=(15,5))\n",
        "for i in range(numex):\n",
        "    example = test_dataset[i]\n",
        "    utils.imshow(example[0],ax[0][i])\n",
        "    utils.imshow(example[1],ax[1][i])\n",
        "    distance = model(example[0].reshape(1,3,64,64),example[1].reshape(1,3,64,64))\n",
        "    ax[0][i].set_title('label:{},distance:{:.2}'.format(example[2].item(),distance.item()))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcwX84fJplle"
      },
      "outputs": [],
      "source": [
        "threshs = [1]\n",
        "accs = []\n",
        "sensitivity = []\n",
        "specificity = []\n",
        "f1=[]\n",
        "\n",
        "\n",
        "for thresh in threshs:\n",
        "    pos=0\n",
        "    neg=0\n",
        "    conf = np.zeros((2,2))\n",
        "    for i,(img1,img2,label) in enumerate(dataloaders['test']):\n",
        "        with torch.no_grad():\n",
        "            out = model(img1,img2)\n",
        "        \n",
        "        conf[1,1] += torch.matmul(label.T,(out>=thresh)*1)\n",
        "        conf[1,0] += torch.matmul(label.T,(out<thresh)*1)\n",
        "        conf[0,1] += torch.matmul((1-label).T,(out>=thresh)*1)\n",
        "        conf[0,0] += torch.matmul((1-label).T,(out<thresh)*1)\n",
        "        pos = pos + torch.sum(label)\n",
        "        neg = neg + torch.sum(1-label)\n",
        "\n",
        "    accs.append(((conf[1,1]+conf[0,0])/(pos+neg)).item())\n",
        "    sensitivity.append((conf[0,0]/neg).item()) #changed 0 and 1 labels (0 is true in the loss fcn)\n",
        "    specificity.append((conf[1,1]/pos).item())\n",
        "    f1.append(((2*conf[0,0])/(2*conf[0,0]+conf[0,1]+conf[1,0])).item())\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wp6r0gBJplle"
      },
      "outputs": [],
      "source": [
        "accs = np.array(accs)\n",
        "sensitivity = np.array(sensitivity)\n",
        "specificity = np.array(specificity)\n",
        "f1 = np.array(f1)\n",
        "\n",
        "m1 = np.argmax(accs)\n",
        "m2 = np.argmax(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "cFtVNn7kplle",
        "outputId": "c3524574-5970-40d0-a66f-b528ee59a423"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(1,3,figsize=(15,4))\n",
        "ax[0].plot(threshs,accs)\n",
        "ax[0].set_xlabel('threshold')\n",
        "ax[0].set_ylabel('accuracy')\n",
        "ax[1].plot(threshs,f1)\n",
        "ax[1].set_xlabel('threshold')\n",
        "ax[1].set_ylabel('f1-score')\n",
        "ax[2].plot(1-specificity,sensitivity,'o',label='_nolabel_')\n",
        "ax[2].set_xlabel('1-specificity')\n",
        "ax[2].set_ylabel('sensitivity')\n",
        "ax[2].plot(1-specificity[m1],sensitivity[m1],'ro',label='acc thresh:{}'.format(threshs[m1]))\n",
        "ax[2].plot(1-specificity[m2],sensitivity[m2],'go',label='f1 thresh:{}'.format(threshs[m2]))\n",
        "ax[2].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GjUqF_cplle"
      },
      "source": [
        "## Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "B-q-iEBJpllf"
      },
      "outputs": [],
      "source": [
        "#torch.save(model.state_dict(),config['MODEL']['modelpath'])\n",
        "model = model.to('cpu')\n",
        "torch.save(model.state_dict(),\"models\\\\mobilenet_triplet_state_dict_50cls.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exctract embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(\"models\\\\mobilenet_triplet_state_dict_50cls.pkl\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_embs = []\n",
        "train_labels = []\n",
        "val_embs = []\n",
        "val_labels = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i,(img1,img2,img3) in enumerate(dataloaders['train']):\n",
        "    img1 = img1.to(device)\n",
        "    img2 = img2.to(device)\n",
        "    img3 = img3.to(device)\n",
        "    with torch.no_grad():\n",
        "        out1 = model.getEmb(img1,img2)\n",
        "        out2 = model.getEmb(img1,img3)\n",
        "\n",
        "    train_embs+=out1.tolist()\n",
        "    train_embs+=out2.tolist()\n",
        "    train_labels+=np.ones((img1.shape[0],)).tolist()\n",
        "    train_labels+=np.zeros((img1.shape[0],)).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i,(img1,img2,img3) in enumerate(dataloaders['val']):\n",
        "    img1 = img1.to(device)\n",
        "    img2 = img2.to(device)\n",
        "    img3 = img3.to(device)\n",
        "    with torch.no_grad():\n",
        "        out1 = model.getEmb(img1,img2)\n",
        "        out2 = model.getEmb(img1,img3)\n",
        "\n",
        "    val_embs+=out1.tolist()\n",
        "    val_embs+=out2.tolist()\n",
        "    val_labels+=np.ones((img1.shape[0],)).tolist()\n",
        "    val_labels+=np.zeros((img1.shape[0],)).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save((train_embs,train_labels),'trainembs_triplet.pkl')\n",
        "torch.save((val_embs,val_labels),'valembs_triplet.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FxpLSmPKplld",
        "-GjUqF_cplle"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "MLenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "665aa01f3af077fd5771c858d8fc10596be116db06e0fbc8e2a6e92782d15ae1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
